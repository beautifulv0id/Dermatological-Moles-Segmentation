{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/beautifulv0id/Dermatological-Moles-Segmentation/blob/main/Dermatological_moles_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jupyter notebook --no-browser --port=8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbywtXs85SKW"
   },
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHcNsjWa5KS0",
    "outputId": "4c507d67-d55d-4d91-e53b-414c1f902872"
   },
   "outputs": [],
   "source": [
    "#!wget -N https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Training_Data.zip\n",
    "#!wget -N https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Training_GroundTruth.zip\n",
    "#!unzip -n ISBI2016_ISIC_Part1_Training_Data\n",
    "#!unzip -n ISBI2016_ISIC_Part1_Training_GroundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print('Error: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz8hMXdW7zLI"
   },
   "source": [
    "## Prepare paths of input images and target segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05ZhKOtF75Wo",
    "outputId": "62bac9ca-83e0-4531-94ef-72a0028bf6bb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_dir = \"data/ISBI2016_ISIC_Part1_Training_Data/\"\n",
    "target_dir = \"data/ISBI2016_ISIC_Part1_Training_GroundTruth/\"\n",
    "img_size = (160, 160)\n",
    "num_classes = 1\n",
    "batch_size = 32\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCvMJc2685t0"
   },
   "source": [
    "## What does one input image and corresponding segmentation mask look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PEYRoIxm88fp",
    "outputId": "b3781d05-23aa-4bf2-9ee0-c16085063f66"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Display input image #7\n",
    "display(Image(filename=input_img_paths[9]))\n",
    "\n",
    "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EvDmrBNBv2v"
   },
   "source": [
    "## Test data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "MxZCGRpd8UaD",
    "outputId": "2936549d-93a1-4b81-9bd6-03a44e46677e"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "input_data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\", seed=10),\n",
    "  tf.keras.layers.RandomRotation(180, seed=10),\n",
    "  tf.keras.layers.Resizing(img_size[0], img_size[1], interpolation=\"bilinear\"),\n",
    "])\n",
    "output_data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\", seed=10),\n",
    "  tf.keras.layers.RandomRotation(180, seed=10),\n",
    "  tf.keras.layers.Resizing(img_size[0], img_size[1], interpolation=\"bilinear\"),\n",
    "])\n",
    "\n",
    "img = load_img(input_img_paths[9])\n",
    "img = tf.convert_to_tensor(tf.keras.preprocessing.image.img_to_array(img))\n",
    "\n",
    "target = load_img(target_img_paths[9])\n",
    "target = tf.convert_to_tensor(tf.keras.preprocessing.image.img_to_array(target))\n",
    "\n",
    "img_out = input_data_augmentation(img, training=True)\n",
    "target_out = output_data_augmentation(target, training=True)\n",
    "\n",
    "display(Image.fromarray(np.uint8(img_out.numpy())))\n",
    "display(Image.fromarray(np.uint8(target_out.numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvxbPi8e9EmQ"
   },
   "source": [
    "## Prepare `Sequence` class to load & vectorize batches of data\n",
    "\n",
    "Currently images are all scaled to size `(160, 160)` even though original input images are much bigger. Furthermore, had to adjust script to get correct labels for ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikPrprYa9GmB"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "class DermatologicalMoles(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths, training=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "        self.training = training\n",
    "        self.input_data_augmentation = Sequential([\n",
    "          layers.RandomFlip(\"horizontal_and_vertical\", seed=123),\n",
    "          layers.RandomZoom(0.1, seed=123),\n",
    "          layers.RandomRotation(90, seed=123),\n",
    "          #layers.Resizing(img_size[0], img_size[1], interpolation=\"bilinear\")\n",
    "        ])\n",
    "        self.output_data_augmentation = Sequential([\n",
    "          layers.RandomFlip(\"horizontal_and_vertical\", seed=123),\n",
    "          layers.RandomZoom(0.1, seed=123, interpolation=\"nearest\"),\n",
    "          layers.RandomRotation(90, seed=123, interpolation=\"nearest\"),\n",
    "          #layers.Resizing(img_size[0], img_size[1],  interpolation=\"nearest\")\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "    \n",
    "    def augment_inputs(self, x):\n",
    "      x = img_to_array(x)\n",
    "      x = tf.convert_to_tensor(x)\n",
    "      x = self.input_data_augmentation(x)\n",
    "      # x = tf.image.resize(\n",
    "      #   x,\n",
    "      #   self.img_size, # (width, height)\n",
    "      #   method=tf.image.ResizeMethod.BILINEAR,\n",
    "\n",
    "      # )\n",
    "      x = x.numpy()\n",
    "      return x\n",
    "\n",
    "    def augment_outputs(self, x):\n",
    "          x = img_to_array(x)\n",
    "          x = tf.convert_to_tensor(x)\n",
    "          x = self.output_data_augmentation(x)\n",
    "          # x = tf.image.resize(\n",
    "          #   x,\n",
    "          #   self.img_size, # (width, height)\n",
    "          #   antialias=False, # Ignored when using NEAREST_NEIGHBOR\n",
    "          #   method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "          # )\n",
    "          x = x.numpy()\n",
    "          return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size = self.img_size)\n",
    "            if self.training:\n",
    "              img = self.augment_inputs(img)\n",
    "            else:\n",
    "              img = tf.keras.preprocessing.image.smart_resize(img, self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size = self.img_size, color_mode=\"grayscale\")\n",
    "            if self.training:\n",
    "              img = self.augment_outputs(img)\n",
    "            else:\n",
    "              img = np.expand_dims(img, 2)\n",
    "              img = tf.keras.preprocessing.image.smart_resize(img, self.img_size, interpolation='nearest')\n",
    "            y[j] = img\n",
    "            # Ground truth labels are 0, 255. Add one to make them 0, 1:\n",
    "            y[j] += 1\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I62ZOnzK-OHH"
   },
   "source": [
    "## Prepare U-Net Xception-style model\n",
    "\n",
    "Change `softmax` activation to `sigmoid` for binary classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFcPiq9W-TF-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel sigmoid classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5Gb3-Pf-ZNS"
   },
   "source": [
    "## Set aside a validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCnx0W70-e0t"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "val_samples = 50\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = DermatologicalMoles(\n",
    "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
    ")\n",
    "val_gen = DermatologicalMoles(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score, accuracy, confusion_matrix\n",
    "from scipy.spatial.distance import dice\n",
    "\n",
    "class MatricsCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Computes the Jaccard score and logs the results to TensorBoard.\"\"\"\n",
    "\n",
    "    def __init__(self, model, x_eval, y_eval, log_dir):\n",
    "        self.model = model\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "        self.jaccard_metric = tf.keras.metrics.Mean(\"jaccard_score\")\n",
    "        self.accuracy_metric = tf.keras.metrics.Mean(\"accuracy\")\n",
    "        self.specificity_metric = tf.keras.metrics.Mean(\"specificity\")\n",
    "        self.sensitivity_metric = tf.keras.metrics.Mean(\"sensitivity\")\n",
    "        self.dice_metric = tf.keras.metrics.Mean(\"dice_score\")\n",
    "        self.epoch = 0\n",
    "        self.summary_writer = tf.summary.create_file_writer(\n",
    "            os.path.join(log_dir, model.name)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        self.epoch += 1\n",
    "        self.jaccard_metric.reset_state()\n",
    "        self.accuracy_metric.reset_state()\n",
    "        self.specificity_metric.reset_state()\n",
    "        self.sensitivity_metric.reset_state()\n",
    "        self.dice_metric.reset_state()\n",
    "        predictions = self.model.predict(self.x_eval)\n",
    "        predictions = np.where(predictions > 0.5, 0, 1)\n",
    "        tn, fp, fn, tp  = confusion_matrix(self.y_eval.flatten(), predictions.flatten(), normalize=False).ravel()\n",
    "        sensivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        dice = (2*tp) / (2*tp+fp+fn)\n",
    "        jaccard = tp / (tp + fp + fn)\n",
    "        self.sensitivity_metric.update_state(sensivity)\n",
    "        self.specificity_metric.update_state(specificity)\n",
    "        self.accuracy_metric.update_state(accuracy)\n",
    "        self.dice_metric.update_state(dice)\n",
    "        self.jaccard_metric.update_state(jaccard)\n",
    "        self._write_metric(\n",
    "            self.sensitivity_metric.name, self.sensitivity_metric.result().numpy().astype(float)\n",
    "        )        \n",
    "        self._write_metric(\n",
    "            self.specificity_metric.name, self.specificity_metric.result().numpy().astype(float)\n",
    "        )\n",
    "        self._write_metric(\n",
    "            self.accuracy_metric.name, self.accuracy_metric.result().numpy().astype(float)\n",
    "        )        \n",
    "        self._write_metric(\n",
    "            self.sensitivity_metric.name, self.sensitivity_metric.result().numpy().astype(float)\n",
    "        )\n",
    "        self._write_metric(\n",
    "            self.jaccard_metric.name, self.jaccard_metric.result().numpy().astype(float)\n",
    "        )\n",
    "\n",
    "    def _write_metric(self, name, value):\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar(\n",
    "                name, value, step=self.epoch,\n",
    "            )\n",
    "            self.summary_writer.flush() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWMhghUc-utF"
   },
   "source": [
    "## Train the model\n",
    "Since we are in normal classification setting use `binary_crossentropy` for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "044ho3j7-ytg",
    "outputId": "1422d72d-efe4-4a59-a47d-203ee5c58517"
   },
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "# We use the binary_crossentropy because our target data binary.\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('training.log')\n",
    "   \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"mole_segmentation.h5\", save_best_only=True),\n",
    "    csv_logger\n",
    "]\n",
    "\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 45\n",
    "with tf.device('/device:GPU:3'):\n",
    "    model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQEbNDNdJN3l"
   },
   "source": [
    "## Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHX_2GAnJNIP"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all images in the validation set\n",
    "\n",
    "val_gen = DermatologicalMoles(batch_size, img_size, val_input_img_paths, val_target_img_paths, training=False)\n",
    "val_preds = model.predict(val_gen)\n",
    "\n",
    "\n",
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    pred = val_preds[i]\n",
    "    mask = np.where(pred > 0.5, 0, 1)\n",
    "    img = ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    display(img)\n",
    "\n",
    "\n",
    "# Display results for validation image #10\n",
    "i = 10\n",
    "\n",
    "# Display input image\n",
    "\n",
    "display(load_img(val_input_img_paths[i], target_size=img_size))\n",
    "\n",
    "# Display ground-truth target mask\n",
    "img = ImageOps.autocontrast(load_img(val_target_img_paths[i], target_size=img_size))\n",
    "display(img)\n",
    "\n",
    "# Display mask predicted by our model\n",
    "display_mask(i)  # Note that the model only sees inputs at 160x160."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYNnPYkwvSdA"
   },
   "source": [
    "## Evaluation\n",
    "Considered metrics computed are: `sensitivity`, `specificity`, `accuracy`, `Jaccard index` and `Dice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-IeD9PiK_Zc"
   },
   "outputs": [],
   "source": [
    "val_target = load_img(val_input_img_paths[0])\n",
    "print(val_preds[0].shape)\n",
    "preds_orig_size = keras.preprocessing.image.smart_resize(val_preds[0], val_target.size)\n",
    "print(preds_orig_size.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsJvebj1Phu9"
   },
   "outputs": [],
   "source": [
    "p = np.array([[1, 1, 0]])\n",
    "v = np.array([[0, 1, 1]])\n",
    "tmp = v[np.where(p == 1)]\n",
    "tp = np.count_nonzero(tmp)\n",
    "fp = len(tmp) - tp\n",
    "print(tp)\n",
    "print(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXnyTOggSX9z"
   },
   "outputs": [],
   "source": [
    "fp = 0.\n",
    "fn = 0.\n",
    "tn = 0.\n",
    "tp = 0.\n",
    "for i, pred in enumerate(val_preds):\n",
    "  target = np.array(load_img(val_target_img_paths[i], color_mode=\"grayscale\"), dtype=\"uint8\")\n",
    "  target += 1\n",
    "  target = keras.preprocessing.image.img_to_array(target)\n",
    "  pred_orig_size = keras.preprocessing.image.smart_resize(pred, target.shape[:-1], interpolation='bilinear')\n",
    "  pred_mask = np.where(pred_orig_size > 0.5, 0, 1)\n",
    "  tmp = target[np.where(pred_mask == 1)]\n",
    "  tp += np.count_nonzero(tmp==1)\n",
    "  fp += np.count_nonzero(tmp==0)\n",
    "  tmp = target[np.where(pred_mask == 0)]\n",
    "  fn += np.count_nonzero(tmp==1)\n",
    "  tn += np.count_nonzero(tmp==0)\n",
    "\n",
    "sensivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "dice = (2*tp) / (2*tp+fp+fn)\n",
    "jaccard = tp / (tp + fp + fn)\n",
    "\n",
    "print(\"sensivity: \", sensivity)\n",
    "print(\"specificity: \", specificity)\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"dice: \", dice)\n",
    "print(\"jaccard: \", jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3NpTRexv6tW"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPpzbOqOb5xo9rnZ/5FDHMw",
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
